{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\Logo_UCLL_ENG_RGB.png\" style=\"background-color:white;\" />\n",
    "\n",
    "# Data Analytics & Machine learning\n",
    "\n",
    "Lecturers: Aimée Lynn Backiel, Chidi Nweke, Daan Nijs\n",
    "\n",
    "Academic year 2023-2024\n",
    "\n",
    "## Lab 5: Machine learning, part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture outline\n",
    "\n",
    "1. Recap of previous weeks\n",
    "2. Intro to machine learning using scikit-learn\n",
    "   1. Preprocessing\n",
    "      1. One Hot encoding\n",
    "      2. Scaling\n",
    "      3. Outliers\n",
    "   2. Classification and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of last lecture(s)\n",
    "\n",
    "#### Lab 1\n",
    "\n",
    "1. We ensured we had a valid Python installation.\n",
    "2. We learnt what a virtual environment is:\n",
    "   * Isolated Python executable and packages.\n",
    "   * We created a virtual environment.\n",
    "3. Absolute path vs relative path recap.\n",
    "4. Recap of data structures in Python\n",
    "\n",
    "#### Lab 2\n",
    "1. Installed Pandas\n",
    "2. Learnt how to read data\n",
    "3. Learnt how to calculate mean, mode, median etc.\n",
    "4. Basic exploration of the 4 variables\n",
    "\n",
    "#### Lab 3\n",
    "1. Wrapped up computing summary statistics (mean, median, mode, ...)\n",
    "2. Learnt how to deal with outliers \n",
    "3. Focused on exploration of dat\n",
    "\n",
    "#### Lab 3\n",
    "1. Univariate data visualization using Matplotlib\n",
    "   1. Figures and axes\n",
    "   2. Histograms\n",
    "   3. Box plots\n",
    "   4. Bar charts\n",
    "2. Multivariate data visualization using Seaborn\n",
    "   1. Scatter plots\n",
    "   2. Small multiples\n",
    "   3. Color coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ada Turing Travelogue, or as everyone calls her, Ada just started working part time at her parents travel agency. She has a keen understanding and interest of everything related to applied computer science ranging from server & system management to full stack software development. Through database foundations she already understands how to query data and programming 1 and 2 covered the essentials about the Python programming language. Recently she has just decided to start learning about data analytics & machine learning as well.\n",
    "\n",
    "She uses her skills to connect to the travel agency's database where she finds many, normalized, tables. Ada recalls what she learnt in database foundations and performs all the correct joins. Afterwards she saves the data in the `data/` folder.\n",
    "\n",
    "\n",
    "She finds the following dataset:\n",
    "\n",
    "| Column Name          | Description                                                                                       |\n",
    "| -------------------- | ------------------------------------------------------------------------------------------------- |\n",
    "| SalesID              | Unique identifier for each sale.                                                                  |\n",
    "| Age                  | Age of the traveler.                                                                              |\n",
    "| Country              | Country of origin of the traveler.                                                                |\n",
    "| Membership_Status    | Membership level of the traveler in the booking system; could be 'standard', 'silver', or 'gold'. |\n",
    "| Previous_Purchases   | Number of previous bookings made by the traveler.                                                 |\n",
    "| Destination          | Travel destination chosen by the traveler.                                                        |\n",
    "| Stay_length          | Duration of stay at the destination.                                                              |\n",
    "| Guests               | Number of guests traveling (including the primary traveler).                                             |\n",
    "| Travel_month         | Month in which the travel is scheduled.                                                           |\n",
    "| Months_before_travel | Number of months prior to travel that the booking was made.                                       |\n",
    "| Earlybird_discount   | Boolean flag indicating whether the traveler received an early bird discount.                     |\n",
    "| Package_Type         | Type of travel package chosen by the traveler.                                                    |\n",
    "| Cost                 | Calculated cost of the travel package.                                                            |\n",
    "| Margin | The cost (for the traveler) - what the travel agency pays. |\n",
    " | Additional_Services_Cost| The amount of additional services (towels, car rentals, room service, ...) that was bought during the trip. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our challenge\n",
    "\n",
    "Before getting into harder use cases we will start off by predicting the cost of a given stay. Right now Ada's parents do this manually automating this task would already be a big help to their business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.datascience-pm.com/wp-content/uploads/2021/02/CRISP-DM.png\" style=\"background-color:white;width:50%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also helps to situate our progress within CRISP-DM. We have done the first three steps, as from this lecture we will progress to modeling. As mentioned in the lecture, this is an iterative procedure, as we are doing modeling we need to circle back to both data preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning with sci-kit learn\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/doc/logos/1280px-scikit-learn-logo.png\" style=\"background-color:white;width:50%\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn is a cornerstone in the Python machine learning toolkit, especially when dealing with tabular data. It's the go-to for scenarios where we're not tackling unstructured data types like text or images but focusing on more classic, structured datasets.\n",
    "\n",
    "The library's compatibility with Pandas and Numpy is seamless, forming a robust stack for data manipulation and analysis. Scikit-learn's API is designed for consistency and ease of use, which streamlines the process of building and deploying machine learning models. It's not just about model creation; the library also provides essential tools for model evaluation and selection, ensuring we can measure and improve our approach systematically.\n",
    "\n",
    "With scikit-learn, you have access to a broad array of algorithms for various machine learning tasks—classification, regression, clustering, and more. It's also equipped with features for preprocessing data, selecting the right features, and fine-tuning models through cross-validation and hyperparameter optimization.\n",
    "\n",
    "In this lab, we'll leverage scikit-learn's features to build effective machine learning models, delve into model evaluation, and explore hyperparameter tuning. We'll work through the key steps of a machine learning pipeline, applying these concepts to our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💻📊💡 Sci-kit learn has fantastic and approachable documentation. We highly encourage you to consult it while doing your exercises. It's split in a reference and a user guide. Just reading the user guide in order, from start to finish, can teach you a lot about machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install\n",
    "# %pip install -U scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # by convention\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "travel_dataset = pd.read_csv(\"data/lab_5_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In traditional programming, incorrect code often results in errors that prevent execution. In contrast, machine learning models can produce silently incorrect results — they'll run without error, but their output may be flawed. This subtlety is crucial and will be a recurring theme in our course.\n",
    "\n",
    "The diagram presented here outlines the comprehensive methodology we'll adopt throughout this class. It's the backbone of a robust machine learning workflow. While the steps might not be immediately clear as we start, our goal is to demystify each stage, making the process second nature to you. By the end of this course, not only will this methodology be intuitive, but it will also be an essential part of your toolkit, especially for your exam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_workflow.png\" style=\"background-color:white;width:50%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our first step: splitting our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting your dataset correctly is the first critical step in a machine learning project. We partition the data into two distinct sets: one for training and another for testing. By training our model on the first set and evaluating it on the second, we aim to create a model that generalizes. A model that **generalizes** has effectively learned the underlying patterns in the dataset rather than just memorizing the training data. This skill is essential to avoid **overfitting**, where a model might excel with the training data but fail to predict accurately on new, unseen data. \n",
    "\n",
    "Throughout this course, we'll focus on how to achieve generalization to ensure that our models are robust and perform well in practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Can you explain what underfitting and overfitting is on the basis of this image? HINT: always try and use the world generalize and generalization in the explanations.\n",
    "##### 💡 Understanding this might be key for the exam..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://www.mathworks.com/discovery/overfitting/_jcr_content/mainParsys/image.adapt.full.medium.svg/1686825007300.svg\" style=\"background-color:white;width:50%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ HARDER: Can you explain what underfitting and overfitting is on the basis of this image? HINT: capacity means that the model is more complex.\n",
    "##### 💡 Understanding this might be key for the exam..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://www.researchgate.net/profile/Felix_Kolodziejczyk/publication/349608675/figure/fig16/AS:995214562955264@1614289050012/Typical-relationship-for-capacity-and-generalization-error-for-a-common-learning-task.ppm\" style=\"background-color:white;width:50%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting with sci-kit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split off 80 % of our data into a training set. The remaining 20 % will serve as our test set. We will also use a **seed** to ensure our split is reproducible. This means that each time we run our script from top to bottom the split is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "import numpy  as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = travel_dataset.drop(columns=\"cost\") # by convention the variables are called (capital) X\n",
    "y = travel_dataset[\"cost\"] # By convention what you want to predict is called (small letter) y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(X_train) = } and {len(X_test) = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is normally part of data understanding. It's an important step so we'll practice it a bit more.  \n",
    "\n",
    "💻📊💡 During the exam we expect you to be able to make these types of plots. They're provided to you here in the interest of time.\n",
    "\n",
    "💻📊💡 We'll be using a third data visualization library called Plotly. We will not explicitly cover the syntax but we encourage you to explore it: https://plotly.com/python/plotly-express/ and use it as the graphs are interactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_train.merge(y_train, right_index=True,left_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(train[[\"cost\",\"age\", \"stay_length\", \"months_before_travel\", \"guests\"]].corr().round(3), title=\"correlation heatmap\", width=750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(train, x=\"country\", y=\"cost\", histfunc=\"avg\", title=\"Average cost per country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(train, x=\"destination\", y=\"cost\", histfunc=\"avg\", title=\"Average cost per destination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(train, x=\"country\", y=\"cost\", histfunc=\"avg\", title=\"Average cost per country, by destination (1)\", color=\"destination\", barmode=\"group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(train, x=\"country\", y=\"cost\", histfunc=\"avg\", title=\"Average cost per country, by destination (2)\", facet_col=\"destination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(train, x=\"package_Type\", title=\"Distribution of the package\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(train, x=\"package_Type\", y = \"cost\", histfunc=\"avg\", color=\"destination\", barmode=\"group\", title=\"Distribution of the package\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(train, x=\"age\", y=\"cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Are there interesting patterns you can spot already?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The patterns you have spotted so far are definitely variables we will include in our machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing recipes: scaling and one hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the lecture, there are two key steps preceding most machine learning algorithms. \n",
    "\n",
    "Firstly, our machine learning models (nearly) always require numeric input. We must convert our text based columns into numbers. The most straight forward way to do it is **one hot encoding.**\n",
    "\n",
    "<center>\n",
    "<b>One hot encoding</b>\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"https://i.imgur.com/mtimFxh.png\" style=\"background-color:white;width:50%\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ On what columns do we need to apply this transformation? Do these types of columns have a name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not boilerplate you typically need. It is sadly necessary for demonstration purposes\n",
    "# We will see easier ways for calling this code in a second\n",
    "ohe = OneHotEncoder(sparse_output=False) \n",
    "ohe.fit_transform(X_train[\"package_Type\"].to_numpy().flatten().reshape(-1,1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💻📊💡 sci-kit learn typically uses a common interface across all its functionality:\n",
    "\n",
    "1.  `Fit` means \"prepare\" this object. For instance, we `fit` a machine learning model, that means its trained and we can use it later. We also `fit` a one hot encoder. We prepare it on existing data. For our `OneHotEncoder` fit essentially does two things: it finds all existing columns and assigns them a number.\n",
    "2.  `transform` means to apply a preprocessing transformation. Typically you need to have `fit` it first. \n",
    "3.  `fit_transform` applies a `fit` and a `transform` in one go.\n",
    "4.  `predict` makes a prediction with a machine learning model.\n",
    "5.  `fit_predict` is analogous to `fit_transform`, it trains a machine learning model and then instantly makes a prediction on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"package_Type\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Interpret the meaning of the numbers in the one hot encoding output you see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = [\"package_Type\", \"destination\", \"country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse_output=True)\n",
    "ohe.fit(X_train[cat_columns])\n",
    "cat_cols_train  = ohe.transform(X_train[cat_columns])\n",
    "cat_cols_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the rest of the demo we need the non-sparse format\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "ohe.fit(X_train[cat_columns])\n",
    "cat_cols_train  = ohe.transform(X_train[cat_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Don't be afraid of the output above. What does it mean? How do you interpret it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ HARDER: can you see the dangerous thing about one hot encoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard scaling is a second step that is typically applied to our data. As mentioned in the lecture, having data that is on different scales can severely impact the performance of machine learning models. Not all models are sensitive to this. In the interest of time we recommend you apply it to *all* models as doing so when not necessary typically does not harm performance, however forgetting it does.\n",
    "\n",
    "**Important: we do not need to standard scale the quantity we are trying to predict!**\n",
    "\n",
    "<center>\n",
    "<b>Standard scaling</b>\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"https://i.stack.imgur.com/QEPAU.png\" style=\"background-color:white;width:50%\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ On what columns do we need to apply this transformation? Do these types of columns have a name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not boilerplate you typically need. It is sadly necessary for demonstration purposes\n",
    "scaler = StandardScaler()\n",
    "scaled_age = scaler.fit_transform(X_train[\"age\"].to_numpy().flatten().reshape(-1,1))\n",
    "scaled_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_guests = scaler.fit_transform(X_train[\"guests\"].to_numpy().flatten().reshape(-1,1))\n",
    "scaled_guests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(scaled_age, nbins=5, title=\"The age column after scaling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(scaled_guests, nbins=5, title=\"the guests column after scaling\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ What can you say about how the data looks like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [\"guests\", \"age\", \"stay_length\"]\n",
    "scaler = StandardScaler()\n",
    "num_cols_train = scaler.fit_transform(X_train[numeric_columns])\n",
    "num_cols_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Towards a first machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have completed our preprocessing. The only thing left to do is combine our categorical and numeric data into one and then fit a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = np.hstack((cat_cols_train, num_cols_train)) # hstack is like a join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ What does (40000, 22) mean? If you understand the shapes of this matrix it means you're following along well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_preprocessed, y_train)\n",
    "predictions = lin_reg.predict(X_train_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=predictions, y=y_train, labels={\"x\": \"predicted\", \"y\": \"actual\"}, title=\"predicted versus actual plot\")\n",
    "fig.add_shape(type=\"line\",\n",
    "              x0=-1000, \n",
    "              y0=-1000, \n",
    "              x1=7000, \n",
    "              y1=7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=predictions, y=y_train, marginal_x=\"histogram\", marginal_y=\"histogram\", labels={\"x\": \"predicted\", \"y\": \"actual\"}, title=\"predicted versus actual plot with marginals\")\n",
    "fig.add_shape(type=\"line\",\n",
    "              x0=-1000, \n",
    "              y0=-1000, \n",
    "              x1=7000, \n",
    "              y1=7000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ How do you interpret such a plot? What is it saying? When are predictions good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ What other machine learning models do you remember from class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Do the same analysis and reason about the graphs, is the performance better or worse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Try applying this to the test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
