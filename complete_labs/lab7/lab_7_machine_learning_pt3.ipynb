{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\Logo_UCLL_ENG_RGB.png\" style=\"background-color:white;\" />\n",
    "\n",
    "# Data Analytics & Machine learning\n",
    "\n",
    "Lecturers: Aimée Lynn Backiel, Chidi Nweke, Daan Nijs\n",
    "\n",
    "Academic year 2023-2024\n",
    "\n",
    "## Lab 7: Machine learning, part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture outline\n",
    "\n",
    "1. Recap of previous weeks\n",
    "2. Automating machine learning pipelines with sci-kit learn\n",
    "3. Model evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus of last lab was on using sci-kit learn's tools to made models. On top of that we saw different important performance metrics for regression and how to use them in sci-kit learn. Our overarching goal remains the same: we want to try out various approaches and select the best one afterwards. This lecture we will continue that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of last lecture(s)\n",
    "\n",
    "#### Lab 1\n",
    "\n",
    "1. We ensured we had a valid Python installation.\n",
    "2. We learnt what a virtual environment is:\n",
    "   * Isolated Python executable and packages.\n",
    "   * We created a virtual environment.\n",
    "3. Absolute path vs relative path recap.\n",
    "4. Recap of data structures in Python\n",
    "\n",
    "#### Lab 2\n",
    "1. Installed Pandas\n",
    "2. Learnt how to read data\n",
    "3. Learnt how to calculate mean, mode, median etc.\n",
    "4. Basic exploration of the 4 variables\n",
    "\n",
    "#### Lab 3\n",
    "1. Wrapped up computing summary statistics (mean, median, mode, ...)\n",
    "2. Learnt how to deal with outliers \n",
    "3. Focused on exploration of dat\n",
    "\n",
    "#### Lab 4\n",
    "1. Univariate data visualization using Matplotlib\n",
    "   1. Figures and axes\n",
    "   2. Histograms\n",
    "   3. Box plots\n",
    "   4. Bar charts\n",
    "2. Multivariate data visualization using Seaborn\n",
    "   1. Scatter plots\n",
    "   2. Small multiples\n",
    "   3. Color coding\n",
    "\n",
    "#### Lab 5\n",
    "1. Intro to machine learning using scikit-learn\n",
    "   1. Preprocessing\n",
    "      1. One Hot encoding\n",
    "      2. Scaling\n",
    "      3. Outliers\n",
    "   2. Regression\n",
    "\n",
    "#### Lab 6\n",
    "1. Preprocessing with scikit-learn\n",
    "   1. ColumnTransformer: Apply a transformation to specific columns.\n",
    "   2. Pipeline: Do several transformations after each other\n",
    "2. Evaluation:\n",
    "   1. Why the mean of the error is a bad idea\n",
    "   2. Mean absolute error\n",
    "   3. Mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ada Turing Travelogue, or as everyone calls her, Ada just started working part time at her parents travel agency. She has a keen understanding and interest of everything related to applied computer science ranging from server & system management to full stack software development. Through database foundations she already understands how to query data and programming 1 and 2 covered the essentials about the Python programming language. Recently she has just decided to start learning about data analytics & machine learning as well.\n",
    "\n",
    "She uses her skills to connect to the travel agency's database where she finds many, normalized, tables. Ada recalls what she learnt in database foundations and performs all the correct joins. Afterwards she saves the data in the `data/` folder.\n",
    "\n",
    "\n",
    "She finds the following dataset:\n",
    "\n",
    "| Column Name          | Description                                                                                       |\n",
    "| -------------------- | ------------------------------------------------------------------------------------------------- |\n",
    "| SalesID              | Unique identifier for each sale.                                                                  |\n",
    "| Age                  | Age of the traveler.                                                                              |\n",
    "| Country              | Country of origin of the traveler.                                                                |\n",
    "| Membership_Status    | Membership level of the traveler in the booking system; could be 'standard', 'silver', or 'gold'. |\n",
    "| Previous_Purchases   | Number of previous bookings made by the traveler.                                                 |\n",
    "| Destination          | Travel destination chosen by the traveler.                                                        |\n",
    "| Stay_length          | Duration of stay at the destination.                                                              |\n",
    "| Guests               | Number of guests traveling (including the primary traveler).                                             |\n",
    "| Travel_month         | Month in which the travel is scheduled.                                                           |\n",
    "| Months_before_travel | Number of months prior to travel that the booking was made.                                       |\n",
    "| Earlybird_discount   | Boolean flag indicating whether the traveler received an early bird discount.                     |\n",
    "| Package_Type         | Type of travel package chosen by the traveler.                                                    |\n",
    "| Cost                 | Calculated cost of the travel package.                                                            |\n",
    "| Margin | The cost (for the traveler) - what the travel agency pays. |\n",
    " | Additional_Services_Cost| The amount of additional services (towels, car rentals, room service, ...) that was bought during the trip. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our challenge\n",
    "\n",
    "Before getting into harder use cases we will start off by predicting the cost of a given stay. Right now Ada's parents do this manually automating this task would already be a big help to their business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://www.datascience-pm.com/wp-content/uploads/2021/02/CRISP-DM.png\" style=\"background-color:white;width:50%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning with sci-kit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center>\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_workflow.png\" style=\"background-color:white;width:50%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ What have we done so far of the image below? What stages have we completed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done train test splitting and we have built a few models on the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue from last lecture. We covered two cornerstones of sklearn namely:\n",
    "\n",
    "1. `make_column_transformer`, this allows you to specify preprocessing you want to apply for different columns. E.g., scaling for numeric columns and one hot encoding for categorical.\n",
    "2. `make_pipeline`, this allows you to compose several steps. So our first step could be preprocessing and the second our ML model. A pipeline is an end-to-end object that lets you go from raw data to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### model evaluation using sci-kit learn (Summary of last lab's code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\u0086305\\Documents\\DAML\\DAML course materials\\course-material\\labs\\lab7\\lab_7_machine_learning_pt3.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/u0086305/Documents/DAML/DAML%20course%20materials/course-material/labs/lab7/lab_7_machine_learning_pt3.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m \u001b[39m# by convention\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/u0086305/Documents/DAML/DAML%20course%20materials/course-material/labs/lab7/lab_7_machine_learning_pt3.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pd\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mfloat_format \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/u0086305/Documents/DAML/DAML%20course%20materials/course-material/labs/lab7/lab_7_machine_learning_pt3.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd # by convention\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "import numpy  as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_dataset = pd.read_csv(\"data/lab_7_dataset.csv\")\n",
    "X = travel_dataset.drop(columns=\"cost\") \n",
    "y = travel_dataset[\"cost\"] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = [\"package_Type\", \"destination\", \"country\"]\n",
    "numeric_columns = [\"guests\", \"age\", \"stay_length\"]\n",
    "\n",
    "preprocessing = make_column_transformer(\n",
    "    (StandardScaler(), numeric_columns),\n",
    "    (OneHotEncoder(sparse_output=False), cat_columns),\n",
    "    remainder=\"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_pipe = make_pipeline(preprocessing, LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Answer to previous lab's question (New material as from here)\n",
    " \n",
    " ❓ Use the pipeline approach discussed above and the mean_absolute_error and mean_squared_error for the following models: RandomForestRegressor, HistGradientBoostingRegressor, DecisionTreeRegressor and LinearRegression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the answer to last session's final question. It also serves as a summary of all the new approaches covered there:\n",
    "\n",
    "1. `model_name_pair` contains a list of tuples with different models inside. We loop over them and try them one-by-one.\n",
    "2. `make_pipeline` takes the preprocessing (scaling numeric columns and one hot encoding categorical columns) and immediately places the model behind it. both `.fit` and `.predict` will now both apply the preprocessing and the model in one go.\n",
    "3. Predictions are make on the training and the test set.\n",
    "4. The results are evaluated by means of the MAE and MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 for those interested: `n_jobs = 7` instructs the model to use 7 CPU cores while training. This is a considerable speed-up for random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING random_forest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "The MAE on the training set is 91.4326020995687 and on the test set it is 235.0808060883703\n",
      "The mse on the training set is 124.22135738060179 and on the test set it is 314.0951110477195\n",
      "\n",
      "STARTING gradient boosting\n",
      "--------------------\n",
      "The MAE on the training set is 215.27566294564866 and on the test set it is 221.9883392863986\n",
      "The mse on the training set is 270.5222135184237 and on the test set it is 282.95933184951315\n",
      "\n",
      "STARTING decision tree\n",
      "--------------------\n",
      "The MAE on the training set is 12.197864719583333 and on the test set it is 277.8483461916666\n",
      "The mse on the training set is 51.045890849136754 and on the test set it is 408.31580508143423\n",
      "\n",
      "STARTING linear regression\n",
      "--------------------\n",
      "The MAE on the training set is 307.0123379074999 and on the test set it is 302.02662900999997\n",
      "The mse on the training set is 415.36793401262753 and on the test set it is 406.0298174069804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name_pair = [(\"random_forest\", RandomForestRegressor(n_jobs=7)), (\"gradient boosting\", HistGradientBoostingRegressor()), (\"decision tree\", DecisionTreeRegressor()), (\"linear regression\", LinearRegression()) ]\n",
    "results = []\n",
    "for pair in model_name_pair:\n",
    "    name, model = pair\n",
    "    print(f\"STARTING {name}\")\n",
    "    pipe = make_pipeline(preprocessing, model)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    predictions_train = pipe.predict(X_train)\n",
    "    predictions_test = pipe.predict(X_test)\n",
    "    print(\"-\"*20)\n",
    "    print(f\"The MAE on the training set is {mean_absolute_error(y_train, predictions_train)} and on the test set it is {mean_absolute_error(y_test, predictions_test)}\")\n",
    "    print(f\"The mse on the training set is {mean_squared_error(y_train, predictions_train, squared=False)} and on the test set it is {mean_squared_error(y_test, predictions_test, squared=False)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Comment on the behavior of the models. Are they overfitting? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the decision tree and random forest are both overfitting. There is a large gap between the performance on the test set and the training set.\n",
    "Linear regression is underfitting, the performance on test and train is similarly bad. \n",
    "Gradient boosting is not underfitting nor overfitting. The difference between test and train is minimal, on top of that, its MAE and MSE are better than all the alternatives we have tried."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the best model: feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall in the previous session we made a number of observations:\n",
    "\n",
    "* If you're visiting the same country as you're from the destination seemed to be cheaper\n",
    "* If you're traveling in approximately the same continent it's also cheaper\n",
    "* There might be the case between age and month.\n",
    "* Maybe we should look at age in groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_to_country = {\n",
    "        \"New York\": \"USA\",\n",
    "        \"Rome\": \"Italy\",\n",
    "        \"Paris\": \"France\",\n",
    "        \"Tokyo\": \"Japan\",\n",
    "        \"Cairo\": \"Egypt\",\n",
    "        \"Sydney\": \"Australia\",\n",
    "        \"Rio\": \"Brazil\",\n",
    "        \"Cape Town\": \"South Africa\",\n",
    "    }\n",
    "country_to_continent = {\n",
    "        \"USA\": \"America\",\n",
    "        \"UK\": \"EMEA\",\n",
    "        \"France\": \"EMEA\",\n",
    "        \"Canada\": \"America\",\n",
    "        \"Australia\": \"Asia\",\n",
    "        \"Germany\": \"EMEA\",\n",
    "        \"Spain\": \"EMEA\",\n",
    "        \"Italy\": \"EMEA\",\n",
    "    }\n",
    "destination_to_continent = {\n",
    "        \"New York\": \"America\",\n",
    "        \"Rome\": \"EMEA\",\n",
    "        \"Paris\": \"EMEA\",\n",
    "        \"Tokyo\": \"Asia\",\n",
    "        \"Cairo\": \"EMEA\",\n",
    "        \"Sydney\": \"Asia\",\n",
    "        \"Rio\": \"America\",\n",
    "        \"Cape Town\": \"Africa\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Use Pandas to make a variables to indicate if they traveled to the same country and then the same continent. \n",
    "\n",
    "##### HINT1: Look at [the map method for Pandas series](https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html). \n",
    "\n",
    "##### HINT2: Remember, a series is simply a column so , `df[column]` gives you a series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39087    False\n",
       "30893    False\n",
       "45278    False\n",
       "16398    False\n",
       "13653    False\n",
       "         ...  \n",
       "11284     True\n",
       "44732    False\n",
       "38158    False\n",
       "860      False\n",
       "15795    False\n",
       "Length: 40000, dtype: bool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[\"country\"] == X_train[\"destination\"].map(destination_to_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39087    False\n",
       "30893    False\n",
       "45278    False\n",
       "16398    False\n",
       "13653     True\n",
       "         ...  \n",
       "11284     True\n",
       "44732    False\n",
       "38158     True\n",
       "860      False\n",
       "15795    False\n",
       "Length: 40000, dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[\"country\"].map(country_to_continent) == X_train[\"destination\"].map(destination_to_continent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to add these variables there is some friction. They don't fit into our sci-kit learn `Pipeline` workflow nicely. We could add them to our entire dataset before splitting. Adding variables to the entire dataset is not risk-free. Doing so may lead to the methodological error we spoke about previously **data leakage**. In the scope of this course it's fine to use this approach to *add* variables. We'll briefly show you the more principled way, but you don't need to know this for the exam. It involves creating a custom `Transformer` which we can then compose in our pipeline as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class CountryMapping(TransformerMixin, BaseEstimator):\n",
    "    country_to_continent: dict[str, str]\n",
    "    destination_to_country: dict[str, str]\n",
    "    destination_to_continent: dict[str, str]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        _X = X.copy() # so we don't change our input data frame\n",
    "        _X['country_match'] = _X['country'] == _X['destination'].map(self.destination_to_country)\n",
    "        _X['continent_match'] = _X['country'].map(self.country_to_continent) == _X['destination'].map(self.destination_to_continent)\n",
    "        return _X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_map = CountryMapping(country_to_continent, destination_to_country, destination_to_continent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales_id</th>\n",
       "      <th>age</th>\n",
       "      <th>country</th>\n",
       "      <th>membership_status</th>\n",
       "      <th>previous_purchases</th>\n",
       "      <th>destination</th>\n",
       "      <th>stay_length</th>\n",
       "      <th>guests</th>\n",
       "      <th>travel_month</th>\n",
       "      <th>months_before_travel</th>\n",
       "      <th>earlybird_discount</th>\n",
       "      <th>package_Type</th>\n",
       "      <th>rating</th>\n",
       "      <th>margin</th>\n",
       "      <th>additional_services_cost</th>\n",
       "      <th>country_match</th>\n",
       "      <th>continent_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39087</th>\n",
       "      <td>39088</td>\n",
       "      <td>24</td>\n",
       "      <td>Germany</td>\n",
       "      <td>silver</td>\n",
       "      <td>2</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>5</td>\n",
       "      <td>1404.79</td>\n",
       "      <td>103.43</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30893</th>\n",
       "      <td>30894</td>\n",
       "      <td>34</td>\n",
       "      <td>USA</td>\n",
       "      <td>standard</td>\n",
       "      <td>2</td>\n",
       "      <td>Paris</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Cultural</td>\n",
       "      <td>5</td>\n",
       "      <td>1135.69</td>\n",
       "      <td>345.63</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45278</th>\n",
       "      <td>45279</td>\n",
       "      <td>32</td>\n",
       "      <td>Italy</td>\n",
       "      <td>standard</td>\n",
       "      <td>2</td>\n",
       "      <td>Rio</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>Relaxation</td>\n",
       "      <td>5</td>\n",
       "      <td>199.89</td>\n",
       "      <td>86.47</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16398</th>\n",
       "      <td>16399</td>\n",
       "      <td>53</td>\n",
       "      <td>Germany</td>\n",
       "      <td>silver</td>\n",
       "      <td>6</td>\n",
       "      <td>Rio</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>5</td>\n",
       "      <td>64.28</td>\n",
       "      <td>-55.23</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13653</th>\n",
       "      <td>13654</td>\n",
       "      <td>36</td>\n",
       "      <td>Germany</td>\n",
       "      <td>silver</td>\n",
       "      <td>3</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>Relaxation</td>\n",
       "      <td>5</td>\n",
       "      <td>352.40</td>\n",
       "      <td>-75.80</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>11285</td>\n",
       "      <td>30</td>\n",
       "      <td>Italy</td>\n",
       "      <td>gold</td>\n",
       "      <td>6</td>\n",
       "      <td>Rome</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>Cultural</td>\n",
       "      <td>7</td>\n",
       "      <td>94.40</td>\n",
       "      <td>21.36</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44732</th>\n",
       "      <td>44733</td>\n",
       "      <td>29</td>\n",
       "      <td>Germany</td>\n",
       "      <td>standard</td>\n",
       "      <td>1</td>\n",
       "      <td>Cape Town</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>7</td>\n",
       "      <td>-157.90</td>\n",
       "      <td>109.68</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38158</th>\n",
       "      <td>38159</td>\n",
       "      <td>33</td>\n",
       "      <td>Germany</td>\n",
       "      <td>standard</td>\n",
       "      <td>2</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>6</td>\n",
       "      <td>665.88</td>\n",
       "      <td>166.64</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>861</td>\n",
       "      <td>43</td>\n",
       "      <td>Germany</td>\n",
       "      <td>standard</td>\n",
       "      <td>4</td>\n",
       "      <td>Cape Town</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>Cultural</td>\n",
       "      <td>5</td>\n",
       "      <td>-271.69</td>\n",
       "      <td>136.77</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>15796</td>\n",
       "      <td>36</td>\n",
       "      <td>Canada</td>\n",
       "      <td>gold</td>\n",
       "      <td>6</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>Cultural</td>\n",
       "      <td>7</td>\n",
       "      <td>593.57</td>\n",
       "      <td>166.97</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sales_id  age  country membership_status  previous_purchases  \\\n",
       "39087     39088   24  Germany            silver                   2   \n",
       "30893     30894   34      USA          standard                   2   \n",
       "45278     45279   32    Italy          standard                   2   \n",
       "16398     16399   53  Germany            silver                   6   \n",
       "13653     13654   36  Germany            silver                   3   \n",
       "...         ...  ...      ...               ...                 ...   \n",
       "11284     11285   30    Italy              gold                   6   \n",
       "44732     44733   29  Germany          standard                   1   \n",
       "38158     38159   33  Germany          standard                   2   \n",
       "860         861   43  Germany          standard                   4   \n",
       "15795     15796   36   Canada              gold                   6   \n",
       "\n",
       "      destination  stay_length  guests  travel_month  months_before_travel  \\\n",
       "39087       Tokyo            4       2            12                     5   \n",
       "30893       Paris            4       2             8                     1   \n",
       "45278         Rio            4       3             2                     4   \n",
       "16398         Rio            2       2             1                     1   \n",
       "13653       Cairo            2       1             2                     4   \n",
       "...           ...          ...     ...           ...                   ...   \n",
       "11284        Rome            2       1             6                     3   \n",
       "44732   Cape Town            2       2             2                     3   \n",
       "38158       Cairo            6       2             2                     3   \n",
       "860     Cape Town            1       2             2                     2   \n",
       "15795       Cairo            3       2             1                     7   \n",
       "\n",
       "       earlybird_discount package_Type  rating  margin  \\\n",
       "39087                True    Adventure       5 1404.79   \n",
       "30893               False     Cultural       5 1135.69   \n",
       "45278                True   Relaxation       5  199.89   \n",
       "16398               False    Adventure       5   64.28   \n",
       "13653                True   Relaxation       5  352.40   \n",
       "...                   ...          ...     ...     ...   \n",
       "11284               False     Cultural       7   94.40   \n",
       "44732               False    Adventure       7 -157.90   \n",
       "38158               False    Adventure       6  665.88   \n",
       "860                 False     Cultural       5 -271.69   \n",
       "15795               False     Cultural       7  593.57   \n",
       "\n",
       "       additional_services_cost  country_match  continent_match  \n",
       "39087                    103.43          False            False  \n",
       "30893                    345.63          False            False  \n",
       "45278                     86.47          False            False  \n",
       "16398                    -55.23          False            False  \n",
       "13653                    -75.80          False             True  \n",
       "...                         ...            ...              ...  \n",
       "11284                     21.36           True             True  \n",
       "44732                    109.68          False            False  \n",
       "38158                    166.64          False             True  \n",
       "860                      136.77          False            False  \n",
       "15795                    166.97          False            False  \n",
       "\n",
       "[40000 rows x 17 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_map.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can later add this as a new preprocessing step to our column transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 We briefly make a transformer called `do_nothing_to`. The semantics aren't important as this is something you will rarely need in practice and definitely not on the exam. All it does is make an anonymous function (a function without a name) that takes an input and returns that as output so it effectively does nothing. We make it a transformer by giving it to `FunctionTransformer`.\n",
    "\n",
    "💡 The reason why this is necessary is that our columns transformer is configured to drop all unused columns. We need to \"use\" it for a column to stay in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FunctionTransformer\n",
    "\n",
    "\n",
    "do_nothing_to = FunctionTransformer(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_country = make_column_transformer(\n",
    "    (StandardScaler(), numeric_columns),\n",
    "    (OneHotEncoder(sparse_output=False), cat_columns),\n",
    "    (do_nothing_to,  [\"country_match\", \"continent_match\"]),\n",
    "    remainder=\"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.jmp.com/en_se/statistics-knowledge-portal/what-is-multiple-regression/mlr-with-interactions/_jcr_content/par/styledcontainer_2069/par/lightbox_3be9/lightboxImage.img.png/1548351208495.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider an alternative approach to the problem above using interaction terms. Interaction terms are created by multiplying two variables together, effectively creating a new variable. This is a mathematical way of capturing the 'AND' condition in our data. For example, after one-hot encoding categorical variables like 'Country' and 'Destination', we can generate interaction terms to explore the combined effect of these two features.\n",
    "\n",
    "Suppose we have 'New York', 'Rome', 'Tokyo', and 'Cairo' as categories for 'City' and 'USA', 'Italy', 'Japan', and 'Egypt' for 'Country'. If we create interaction terms for 'City' and 'Country', we end up with additional columns such as 'New York x USA', 'Rome x Italy', and so on. Each of these new columns will have a value of 1 only if both contributing variables (e.g., 'City' is 'New York' AND 'Country' is 'USA') are 1; otherwise, the value will be 0. This new variable thus answers the question: \"Is the traveler from X city AND going to Y country?\"\n",
    "\n",
    "By including interaction terms, we allow our model to consider the combined influence of two variables, which can be particularly insightful when the effect of one variable on the outcome depends on the level of another variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, interaction effects between variables can be encoded using the `PolynomialFeatures` transformer. Interaction effects are valuable in a model when the relationship between two features can affect the outcome in a way that is not simply additive.\n",
    "\n",
    "For example, consider two binary features, A and B. Individually, they might have a certain effect on the target variable Y. However, when both A and B occur together (i.e., A=1 and B=1), their combined effect on Y could be different from the sum of their individual effects. This is where interaction terms come into play.\n",
    "\n",
    "The PolynomialFeatures transformer can not only generate polynomial features, which are features raised to a power (like $x^{2}$ or $x^{3}$), but also interaction features, which are products of features (like $x_{1} * x_{2}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note: polynomial is called \"veelterm\" in Dutch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start off by generating interactions between all our numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(interaction_only=True)\n",
    "\n",
    "numeric_preprocessing = make_pipeline(StandardScaler(), poly)\n",
    "\n",
    "preprocessing_interactions = make_column_transformer(\n",
    "    (numeric_preprocessing, numeric_columns),\n",
    "    (do_nothing_to,  [\"country_match\", \"continent_match\"]),\n",
    "    (OneHotEncoder(sparse_output=False), cat_columns),\n",
    "    remainder=\"drop\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/2000/1*LGTAObYYj2-fdBMFLz30rw.jpeg\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binning is a technique that involves segmenting a continuous variable into several intervals, or 'bins'. Similar to the way we categorize data when creating histograms, binning transforms a continuous variable into an ordered categorical variable. One hot encoding these bins allows us to introduce non-linear effects into our linear models, which ordinarily would interpret the data as having a constant slope.\n",
    "\n",
    "Take temperature as an example: people generally enjoy mild increases in weather warmth, but there's a threshold beyond which higher temperatures become unpleasant. Binning would let us model this non-linear relationship. Instead of treating temperature as a single continuous predictor with a constant effect, we could divide temperatures into ranges (e.g., 0-10°C, 10-20°C, 20-30°C, etc.) and treat each range as a separate category. By one hot encoding these categories, we enable our model to capture the varying effects of different temperature ranges on people's comfort levels. This approach can reveal more complex patterns in how the predictor variable (in this case, temperature) influences the outcome variable (such as people's reported happiness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "\n",
    "preprocessing_bins = make_column_transformer(\n",
    "    (StandardScaler(), numeric_columns),\n",
    "    (KBinsDiscretizer(),  [\"age\"]),\n",
    "    (do_nothing_to,  [\"country_match\", \"continent_match\"]),\n",
    "    (OneHotEncoder(), cat_columns),\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "preprocessing_bins_interactions = make_column_transformer(\n",
    "    (numeric_preprocessing, numeric_columns),\n",
    "    (KBinsDiscretizer(), [\"age\"]),\n",
    "    (do_nothing_to,  [\"country_match\", \"continent_match\"]),\n",
    "    (OneHotEncoder(sparse_output=False), cat_columns),\n",
    "    remainder=\"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the best model: cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we explore various models and preprocessing techniques, we encounter a dilemma: how do we identify the best model without biasing our selection? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ What is the weakness of the train-test split approach? Think about this before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using just one test set might lead us to choose a model that excels on that particular subset of data by sheer luck. What if a different shuffle of the data leads to a different 'best' model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Think about a solution for this before we continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resolve this, let's consider a more robust method. Imagine if we could test each model not on one but multiple randomized slices of our data. This is where cross-validation comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here's how it works: We divide our training set into smaller sections, say 20% chunks. We train our model on 80% of the data, then validate it on the remaining 20%. We repeat this process five times, each time with a different 20% held out for validation. This technique, known as k-fold cross-validation (with k being the number of chunks or 'folds' we create), allows each model a fair shot at proving itself across the entirety of our data.\n",
    "\n",
    "By averaging the performance across these folds, we obtain a more reliable measure of a model's quality. This thorough approach increases our confidence that we're selecting the best model, not by chance, but by consistent performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" style=\"background-color:white\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Sanity check: if we do K-fold cross validation, how many models have we trained. Answer for 2-fold, 3-fold, 5-fold and K-fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2, 3, 5 and K. You make K splits so you train K models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ What is the downside of K-fold cross validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you're training a lot of models it could take a prohibitively long time to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, sci-kit learn makes it easy to do cross validation. \n",
    "\n",
    "We supply `cross_val_score` with 3 mandatory parameters:\n",
    "\n",
    "* The machine learning model\n",
    "* The `X_train`\n",
    "* `X_test`\n",
    "  \n",
    "Additionally you can use `cv` to specify how many folds and you can pick a `score` parameter, which is the result that will be reported to you as the performance on each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-407.54493892, -430.31872512, -415.4150392 , -416.00540587,\n",
       "       -408.91429495])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lin_reg_cv_results = cross_val_score(lin_reg_pipe, X_train, y_train, cv=5, scoring= \"neg_root_mean_squared_error\")\n",
    "lin_reg_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-415.639680810938"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lin_reg_cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_pipe = make_pipeline(numeric_preprocessing, DecisionTreeRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-411.71180971, -423.54799952, -413.5144439 , -407.63445237,\n",
       "       -412.44588413])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_tree_cv_results = cross_val_score(decision_tree_pipe, X_train, y_train, cv=5, scoring= \"neg_root_mean_squared_error\")\n",
    "decision_tree_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-413.7709179244554"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(decision_tree_cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Interpret these values. Which model performs better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression performs better than the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Contrast the performance seen in the beginning of this notebook of these two models. Think about overfitting and so on. (Key Insight!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we evaluated the decision tree on the training set it performed really well. Once we added the test set into the mix it became clear the model was overfitting. By using cross validation we are able to see that linear regression is the better model **without** having to use our test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Your turn: experiment with different models and setups. Try out the new pre processing pipelines with a variety of machine learning models and cross validation. Feel free to reuse some of the evaluation code of the beginning of the lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_country \n",
    "preprocessing_interactions\n",
    "preprocessing_bins\n",
    "preprocessing_bins_interactions;\n",
    "\n",
    "# Your pipeline should look like this: pipe = make_pipeline(country_map, prep, model)\n",
    "# Deviations from this will likely cause an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING random_forest with preprocessing = same country and continent\n",
      "--------------------\n",
      "The MAE on the training set is 90.08049818885435 and on the test set it is 231.75422228752436\n",
      "The mse on the training set is 122.5409192023597 and on the test set it is 308.03027639237615\n",
      "\n",
      "STARTING random_forest with preprocessing = interactions\n",
      "--------------------\n",
      "The MAE on the training set is 90.2064913094094 and on the test set it is 231.37146928853224\n",
      "The mse on the training set is 122.51715614838321 and on the test set it is 307.448897817142\n",
      "\n",
      "STARTING random_forest with preprocessing = bins\n",
      "--------------------\n",
      "The MAE on the training set is 90.2778500380723 and on the test set it is 231.16272497215812\n",
      "The mse on the training set is 122.50225850425151 and on the test set it is 307.0998906937639\n",
      "\n",
      "STARTING random_forest with preprocessing = bins and interactions\n",
      "--------------------\n",
      "The MAE on the training set is 89.9867915547324 and on the test set it is 231.37971949728416\n",
      "The mse on the training set is 122.08072220686789 and on the test set it is 306.8075868912473\n",
      "\n",
      "STARTING gradient boosting with preprocessing = same country and continent\n",
      "--------------------\n",
      "The MAE on the training set is 214.3213887625967 and on the test set it is 220.63287003864832\n",
      "The mse on the training set is 267.5583639965483 and on the test set it is 280.0434214415825\n",
      "\n",
      "STARTING gradient boosting with preprocessing = interactions\n",
      "--------------------\n",
      "The MAE on the training set is 213.55057429977165 and on the test set it is 220.71301646572527\n",
      "The mse on the training set is 266.8810344286796 and on the test set it is 279.78746900884033\n",
      "\n",
      "STARTING gradient boosting with preprocessing = bins\n",
      "--------------------\n",
      "The MAE on the training set is 214.8903572395699 and on the test set it is 220.68333204964824\n",
      "The mse on the training set is 268.53154802882887 and on the test set it is 279.5588115722771\n",
      "\n",
      "STARTING gradient boosting with preprocessing = bins and interactions\n",
      "--------------------\n",
      "The MAE on the training set is 215.19289397971053 and on the test set it is 221.15434110297076\n",
      "The mse on the training set is 269.10625115567365 and on the test set it is 281.5531682134917\n",
      "\n",
      "STARTING decision tree with preprocessing = same country and continent\n",
      "--------------------\n",
      "The MAE on the training set is 12.197864719583333 and on the test set it is 273.54547153166664\n",
      "The mse on the training set is 51.045890849136754 and on the test set it is 403.3935321667544\n",
      "\n",
      "STARTING decision tree with preprocessing = interactions\n",
      "--------------------\n",
      "The MAE on the training set is 12.197864719583333 and on the test set it is 274.35059573416663\n",
      "The mse on the training set is 51.045890849136754 and on the test set it is 407.2374529871902\n",
      "\n",
      "STARTING decision tree with preprocessing = bins\n",
      "--------------------\n",
      "The MAE on the training set is 12.197864719583333 and on the test set it is 273.7737843233333\n",
      "The mse on the training set is 51.045890849136754 and on the test set it is 403.82468510763937\n",
      "\n",
      "STARTING decision tree with preprocessing = bins and interactions\n",
      "--------------------\n",
      "The MAE on the training set is 12.197864719583333 and on the test set it is 276.9199861891667\n",
      "The mse on the training set is 51.045890849136754 and on the test set it is 410.25587349372586\n",
      "\n",
      "STARTING linear regression with preprocessing = same country and continent\n",
      "--------------------\n",
      "The MAE on the training set is 288.9785761175 and on the test set it is 285.82920906000004\n",
      "The mse on the training set is 390.93096976067636 and on the test set it is 383.1413672362504\n",
      "\n",
      "STARTING linear regression with preprocessing = interactions\n",
      "--------------------\n",
      "The MAE on the training set is 285.7474883599999 and on the test set it is 282.72320658999996\n",
      "The mse on the training set is 385.7956123361967 and on the test set it is 378.9172548882946\n",
      "\n",
      "STARTING linear regression with preprocessing = bins\n",
      "--------------------\n",
      "The MAE on the training set is 285.68568731249997 and on the test set it is 283.120271\n",
      "The mse on the training set is 387.692000236871 and on the test set it is 380.3964989367454\n",
      "\n",
      "STARTING linear regression with preprocessing = bins and interactions\n",
      "--------------------\n",
      "The MAE on the training set is 282.73324271750005 and on the test set it is 280.31319098\n",
      "The mse on the training set is 382.52327395410043 and on the test set it is 376.25513105529814\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name_pair = [(\"random_forest\", RandomForestRegressor(n_jobs=7)), (\"gradient boosting\", HistGradientBoostingRegressor()), (\"decision tree\", DecisionTreeRegressor()), (\"linear regression\", LinearRegression()) ]\n",
    "preprocessing = [(\"same country and continent\", preprocessing_country), (\"interactions\", preprocessing_interactions), (\"bins\", preprocessing_bins), (\"bins and interactions\", preprocessing_bins_interactions) ]\n",
    "results = []\n",
    "for pair in model_name_pair:\n",
    "    name, model = pair\n",
    "    for prep_name, prep in preprocessing:\n",
    "        print(f\"STARTING {name} with preprocessing = {prep_name}\")\n",
    "        pipe = make_pipeline(country_map, prep, model)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        predictions_train = pipe.predict(X_train)\n",
    "        predictions_test = pipe.predict(X_test)\n",
    "        print(\"-\"*20)\n",
    "        print(f\"The MAE on the training set is {mean_absolute_error(y_train, predictions_train)} and on the test set it is {mean_absolute_error(y_test, predictions_test)}\")\n",
    "        print(f\"The mse on the training set is {mean_squared_error(y_train, predictions_train, squared=False)} and on the test set it is {mean_squared_error(y_test, predictions_test, squared=False)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Is this an improvement over before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For gradient boosting it's not a noticeable improvement. For linear regression on the other hand it did improve the MSE by approximately 20. This makes sense because linear regression is a very simple model that considers the effect on each of the variables on the target separately. By adding non-linearities such as interactions, binning, and so on. the model is able to improve.\n",
    "\n",
    "Gradient boosting on the other hand is a decision tree algorithm. Specifically, they are an ensemble of decision trees where the next tree is trained to reduce the error of all the previous trees. Trees are capable of handling complex non-linearities by themselves. They perform binning and interactions by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the best model: can we do more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already tried different methods like creating new features and grouping data. These methods have sometimes made our models better. But, there's more we can do.\n",
    "\n",
    "Machine learning is a process that goes in circles, as shown in the CRISP-DM image. Once we build models, there are ways to look more closely at how they work.\n",
    "\n",
    "1. Comparing Predicted and Actual Results\n",
    "   \n",
    "The first step we did was to compare what our model predicted with the real results. This helps us see where the model does well and where it needs more work. It also shows anomalies, for instance certain models were predicting negative costs which should not be possible.\n",
    "\n",
    "2. Looking at Residuals\n",
    "   \n",
    "Here, we look at the errors - the difference between the real values (y_true) and what the model predicted (y_pred). We do this for each input in our model. The main idea is to look for patterns in these errors. For example, if we make charts showing errors for each country and notice that errors are bigger for certain countries, it tells us that we might need to improve our features. This is like doing analysis on the results of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameters and hyperparameters (only theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we can do is called **hyperparameter tuning**. Hyperparameters are simply the \"settings\" of the machine learning model. Parameters are what it uses to make predictions. So for linear regression the parameters are the coefficients and for decision trees these are the splits it is making. \n",
    "\n",
    "Most machine learning models have hyperparameters (settings) as well. Typically they are used to decrease the complexity of the model. For a decision tree a hyperparameter is for instance the amount of splits it is allowed to make. For Random Forest and Gradient boosting the amount of trees it uses are a hyperparameter. For linear regression the regularization constant (punishment if the coefficients get large) is also a hyperparameter.**Hyperparameters help us combat overfitting.**\n",
    "\n",
    "\n",
    "Typically complex models have many different hyperparameters you can tune. We encourage you to look at the number of hyperparameters for the models we have used so far to get an idea:\n",
    "\n",
    "* [Linear regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "* [Gradient boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html)\n",
    "* [Random forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "* [Decision tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n",
    "\n",
    "\n",
    "Some models are very sensitive to their choice of hyperparameters. With their defaults they typically overfit, this is the case for random forest and decision trees (see above).\n",
    "\n",
    "To identify the best hyperparameters, techniques like grid search and random search are used. Grid search exhaustively explores all possible combinations of hyperparameters, offering a thorough but computationally intensive approach. Random search, on the other hand, samples a predetermined number of combinations randomly, providing a quicker but slightly less focused alternative.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.researchgate.net/publication/341691661/figure/fig2/AS:896464364507139@1590745168758/Comparison-between-a-grid-search-and-b-random-search-for-hyper-parameter-tuning-The.png\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ hyperparameters can be sensitive to specific parts of the data. We don't want hyperparameters that do well on one specific part of the data. What can we do to make sure we select hyperparameters that work well for the entire dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each hyperparameter combination we do K-fold cross validation. This means if we have 2 parameters with each 10 outcomes we have 100 combinations which each need K-1 models, assuming you are doing grid search. This is a very expensive procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The key question is if manually doing grid search or random search is worth it.** The answer, as usual, is it depends.\n",
    "\n",
    "Counter-arguments: \n",
    "\n",
    "1. Linear regression in sci-kit learn automatically performs hyperparameter tuning:\n",
    "\n",
    "* [RidgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html)\n",
    "* [ElasticNetCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html)\n",
    "   \n",
    "Their names come from the specific regularization they perform, covering that is out of scope for this course.\n",
    "\n",
    "The CV stands for \"cross validation\". These models perform hyperparameter tuning with cross validation by default. For linear regression this is fine as this is a very inexpensive model to train.\n",
    "\n",
    "2. It is very time consuming.\n",
    "3. The difference can be negligible. You also don't know ahead of time if it will matter.\n",
    "4. It requires you to somewhat know the internal details of the models.\n",
    "5. Gradient boosting is extremely powerful with the default settings, it may not even require tuning.\n",
    "\n",
    "\n",
    "Arguments for:\n",
    "\n",
    "1. In some cases a 1 % difference absolutely makes a difference.\n",
    "\n",
    "This is related to point 3. Even if the difference is negligible at a certain scale a 1 % increase matters. For instance, if you have a ML model to predict how much food will be thrown out for a small grocery store a difference of 1 % does not really matter. If this is on the scale of the entire franchise then it is a worthwhile investment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisiting the complex image we began with, after our lab today, the process should be clearer:\n",
    "\n",
    "<center>\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_workflow.png\" style=\"background-color:white;width:50%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. You start by dividing your dataset into two parts: training and test data.\n",
    "2. The training data is then used for cross-validation. This technique is a reliable way to assess model performance and can be paired with hyperparameter tuning, although that's not mandatory.\n",
    "3. Choose the model or models that perform best in the cross-validation phase.\n",
    "4. These top models are then trained with the entire set of training data.\n",
    "5. finally, we test these models on the test data. The results from this step give us our final evaluation metrics.\n",
    "\n",
    "\n",
    "Remember, this method, aside from the optional step of hyperparameter tuning, is crucial for your exam. Evaluating and training on the same data is a practice we want to avoid because it can lead to overfitting, which is why we test our models on unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
